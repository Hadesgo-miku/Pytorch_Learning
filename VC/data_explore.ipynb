{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zhaohongbo/PycharmProjects/Pytorch_learning/VC\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../data/investments_VC_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "drop_lst=['permalink','homepage_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=drop_lst, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>category_list</th>\n",
       "      <th>market</th>\n",
       "      <th>funding_total_usd</th>\n",
       "      <th>status</th>\n",
       "      <th>country_code</th>\n",
       "      <th>state_code</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "      <th>funding_rounds</th>\n",
       "      <th>...</th>\n",
       "      <th>secondary_market</th>\n",
       "      <th>product_crowdfunding</th>\n",
       "      <th>round_A</th>\n",
       "      <th>round_B</th>\n",
       "      <th>round_C</th>\n",
       "      <th>round_D</th>\n",
       "      <th>round_E</th>\n",
       "      <th>round_F</th>\n",
       "      <th>round_G</th>\n",
       "      <th>round_H</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#waywire</td>\n",
       "      <td>|Entertainment|Politics|Social Media|News|</td>\n",
       "      <td>News</td>\n",
       "      <td>17,50,000</td>\n",
       "      <td>acquired</td>\n",
       "      <td>USA</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York City</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&amp;TV Communications</td>\n",
       "      <td>|Games|</td>\n",
       "      <td>Games</td>\n",
       "      <td>40,00,000</td>\n",
       "      <td>operating</td>\n",
       "      <td>USA</td>\n",
       "      <td>CA</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'Rock' Your Paper</td>\n",
       "      <td>|Publishing|Education|</td>\n",
       "      <td>Publishing</td>\n",
       "      <td>40,000</td>\n",
       "      <td>operating</td>\n",
       "      <td>EST</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tallinn</td>\n",
       "      <td>Tallinn</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(In)Touch Network</td>\n",
       "      <td>|Electronics|Guides|Coffee|Restaurants|Music|iPhone|Apps|Mobile|iOS|E-Commerce|</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>15,00,000</td>\n",
       "      <td>operating</td>\n",
       "      <td>GBR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>London</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-R- Ranch and Mine</td>\n",
       "      <td>|Tourism|Entertainment|Games|</td>\n",
       "      <td>Tourism</td>\n",
       "      <td>60,000</td>\n",
       "      <td>operating</td>\n",
       "      <td>USA</td>\n",
       "      <td>TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>Fort Worth</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name                                                                    category_list        market   funding_total_usd      status country_code state_code         region         city  funding_rounds  ... secondary_market product_crowdfunding round_A  round_B round_C round_D  round_E  round_F  round_G  round_H\n",
       "0            #waywire                                       |Entertainment|Politics|Social Media|News|          News           17,50,000    acquired          USA         NY  New York City     New York             1.0  ...              0.0                  0.0     0.0      0.0     0.0     0.0      0.0      0.0      0.0      0.0\n",
       "1  &TV Communications                                                                          |Games|         Games           40,00,000   operating          USA         CA    Los Angeles  Los Angeles             2.0  ...              0.0                  0.0     0.0      0.0     0.0     0.0      0.0      0.0      0.0      0.0\n",
       "2   'Rock' Your Paper                                                           |Publishing|Education|    Publishing              40,000   operating          EST        NaN        Tallinn      Tallinn             1.0  ...              0.0                  0.0     0.0      0.0     0.0     0.0      0.0      0.0      0.0      0.0\n",
       "3   (In)Touch Network  |Electronics|Guides|Coffee|Restaurants|Music|iPhone|Apps|Mobile|iOS|E-Commerce|   Electronics           15,00,000   operating          GBR        NaN         London       London             1.0  ...              0.0                  0.0     0.0      0.0     0.0     0.0      0.0      0.0      0.0      0.0\n",
       "4  -R- Ranch and Mine                                                    |Tourism|Entertainment|Games|       Tourism              60,000   operating          USA         TX         Dallas   Fort Worth             2.0  ...              0.0                  0.0     0.0      0.0     0.0     0.0      0.0      0.0      0.0      0.0\n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data Wrangler 生成的单元格。\n",
    "\"\"\"\n",
    "def clean_data(df):\n",
    "    # 将缺少的值替换为列: 'name' 中的 \"Unknown\"\n",
    "    df = df.fillna({'name': \"Unknown\"})\n",
    "    # 将缺少的值替换为列: 'category_list' 中的 \"Unknown\"\n",
    "    df = df.fillna({'category_list': \"Unknown\"})\n",
    "    # 将缺少的值替换为列: ' market ' 中的 \"Unknown\"\n",
    "    df = df.fillna({' market ': \"Unknown\"})\n",
    "    # 将缺少的值替换为列: 'status' 中的 \"Unknown\"\n",
    "    df = df.fillna({'status': \"Unknown\"})\n",
    "    # 将缺少的值替换为列: ' funding_total_usd ' 中的 \"0\"\n",
    "    df = df.fillna({' funding_total_usd ': \" -   \"})\n",
    "    return df\n",
    "\n",
    "df_clean = clean_data(df.copy())\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>category_list</th>\n",
       "      <th>market</th>\n",
       "      <th>funding_total_usd</th>\n",
       "      <th>status</th>\n",
       "      <th>country_code</th>\n",
       "      <th>state_code</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "      <th>funding_rounds</th>\n",
       "      <th>...</th>\n",
       "      <th>product_crowdfunding</th>\n",
       "      <th>round_A</th>\n",
       "      <th>round_B</th>\n",
       "      <th>round_C</th>\n",
       "      <th>round_D</th>\n",
       "      <th>round_E</th>\n",
       "      <th>round_F</th>\n",
       "      <th>round_G</th>\n",
       "      <th>round_H</th>\n",
       "      <th>fund_info_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#waywire</td>\n",
       "      <td>|Entertainment|Politics|Social Media|News|</td>\n",
       "      <td>News</td>\n",
       "      <td>17,50,000</td>\n",
       "      <td>acquired</td>\n",
       "      <td>USA</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York City</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&amp;TV Communications</td>\n",
       "      <td>|Games|</td>\n",
       "      <td>Games</td>\n",
       "      <td>40,00,000</td>\n",
       "      <td>operating</td>\n",
       "      <td>USA</td>\n",
       "      <td>CA</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'Rock' Your Paper</td>\n",
       "      <td>|Publishing|Education|</td>\n",
       "      <td>Publishing</td>\n",
       "      <td>40,000</td>\n",
       "      <td>operating</td>\n",
       "      <td>EST</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Tallinn</td>\n",
       "      <td>Tallinn</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(In)Touch Network</td>\n",
       "      <td>|Electronics|Guides|Coffee|Restaurants|Music|iPhone|Apps|Mobile|iOS|E-Commerce|</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>15,00,000</td>\n",
       "      <td>operating</td>\n",
       "      <td>GBR</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>London</td>\n",
       "      <td>London</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-R- Ranch and Mine</td>\n",
       "      <td>|Tourism|Entertainment|Games|</td>\n",
       "      <td>Tourism</td>\n",
       "      <td>60,000</td>\n",
       "      <td>operating</td>\n",
       "      <td>USA</td>\n",
       "      <td>TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>Fort Worth</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name                                                                    category_list        market   funding_total_usd      status country_code state_code         region         city  funding_rounds  ... product_crowdfunding round_A round_B  round_C round_D round_E  round_F  round_G  round_H  fund_info_missing\n",
       "0            #waywire                                       |Entertainment|Politics|Social Media|News|          News           17,50,000    acquired          USA         NY  New York City     New York             1.0  ...                  0.0     0.0     0.0      0.0     0.0     0.0      0.0      0.0      0.0                  0\n",
       "1  &TV Communications                                                                          |Games|         Games           40,00,000   operating          USA         CA    Los Angeles  Los Angeles             2.0  ...                  0.0     0.0     0.0      0.0     0.0     0.0      0.0      0.0      0.0                  0\n",
       "2   'Rock' Your Paper                                                           |Publishing|Education|    Publishing              40,000   operating          EST    Unknown        Tallinn      Tallinn             1.0  ...                  0.0     0.0     0.0      0.0     0.0     0.0      0.0      0.0      0.0                  0\n",
       "3   (In)Touch Network  |Electronics|Guides|Coffee|Restaurants|Music|iPhone|Apps|Mobile|iOS|E-Commerce|   Electronics           15,00,000   operating          GBR    Unknown         London       London             1.0  ...                  0.0     0.0     0.0      0.0     0.0     0.0      0.0      0.0      0.0                  0\n",
       "4  -R- Ranch and Mine                                                    |Tourism|Entertainment|Games|       Tourism              60,000   operating          USA         TX         Dallas   Fort Worth             2.0  ...                  0.0     0.0     0.0      0.0     0.0     0.0      0.0      0.0      0.0                  0\n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data Wrangler 生成的单元格。\n",
    "\"\"\"\n",
    "def clean_data(df_clean):\n",
    "    # 将缺少的值替换为列: 'country_code' 中的 \"Unknown\"\n",
    "    df_clean = df_clean.fillna({'country_code': \"Unknown\"})\n",
    "    # 将缺少的值替换为列: 'state_code', 'region', 'city' 中的 \"Unknown\"\n",
    "    df_clean = df_clean.fillna({'state_code': \"Unknown\", 'region': \"Unknown\", 'city': \"Unknown\"})\n",
    "    df['seed_missing'] = df['seed'].isnull().astype(int)\n",
    "    # 从公式创建的列 'seed_missing'\n",
    "    df_clean['fund_info_missing'] = df_clean['seed'].isnull().astype(int)\n",
    "    # 将缺少的值替换为列: 'venture'、'undisclosed'和其他列16 中的 0\n",
    "    df_clean = df_clean.fillna({'venture': 0, 'undisclosed': 0, 'convertible_note': 0, 'debt_financing': 0, 'angel': 0, 'grant': 0, 'private_equity': 0, 'post_ipo_equity': 0, 'post_ipo_debt': 0, 'secondary_market': 0, 'round_A': 0, 'round_B': 0, 'round_C': 0, 'round_D': 0, 'round_E': 0, 'round_F': 0, 'round_G': 0, 'round_H': 0})\n",
    "    # 将缺少的值替换为列: 'equity_crowdfunding', 'product_crowdfunding', 'seed' 中的 0\n",
    "    df_clean = df_clean.fillna({'equity_crowdfunding': 0, 'product_crowdfunding': 0, 'seed': 0})\n",
    "    return df_clean\n",
    "\n",
    "df_clean_1 = clean_data(df_clean.copy())\n",
    "df_clean_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>category_list</th>\n",
       "      <th>market</th>\n",
       "      <th>funding_total_usd</th>\n",
       "      <th>status</th>\n",
       "      <th>country_code</th>\n",
       "      <th>state_code</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "      <th>funding_rounds</th>\n",
       "      <th>...</th>\n",
       "      <th>product_crowdfunding</th>\n",
       "      <th>round_A</th>\n",
       "      <th>round_B</th>\n",
       "      <th>round_C</th>\n",
       "      <th>round_D</th>\n",
       "      <th>round_E</th>\n",
       "      <th>round_F</th>\n",
       "      <th>round_G</th>\n",
       "      <th>round_H</th>\n",
       "      <th>fund_info_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#waywire</td>\n",
       "      <td>|Entertainment|Politics|Social Media|News|</td>\n",
       "      <td>News</td>\n",
       "      <td>17,50,000</td>\n",
       "      <td>acquired</td>\n",
       "      <td>USA</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York City</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&amp;TV Communications</td>\n",
       "      <td>|Games|</td>\n",
       "      <td>Games</td>\n",
       "      <td>40,00,000</td>\n",
       "      <td>operating</td>\n",
       "      <td>USA</td>\n",
       "      <td>CA</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'Rock' Your Paper</td>\n",
       "      <td>|Publishing|Education|</td>\n",
       "      <td>Publishing</td>\n",
       "      <td>40,000</td>\n",
       "      <td>operating</td>\n",
       "      <td>EST</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Tallinn</td>\n",
       "      <td>Tallinn</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(In)Touch Network</td>\n",
       "      <td>|Electronics|Guides|Coffee|Restaurants|Music|iPhone|Apps|Mobile|iOS|E-Commerce|</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>15,00,000</td>\n",
       "      <td>operating</td>\n",
       "      <td>GBR</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>London</td>\n",
       "      <td>London</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-R- Ranch and Mine</td>\n",
       "      <td>|Tourism|Entertainment|Games|</td>\n",
       "      <td>Tourism</td>\n",
       "      <td>60,000</td>\n",
       "      <td>operating</td>\n",
       "      <td>USA</td>\n",
       "      <td>TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>Fort Worth</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name                                                                    category_list        market   funding_total_usd      status country_code state_code         region         city  funding_rounds  ... product_crowdfunding round_A round_B  round_C round_D round_E  round_F  round_G  round_H  fund_info_missing\n",
       "0            #waywire                                       |Entertainment|Politics|Social Media|News|          News           17,50,000    acquired          USA         NY  New York City     New York             1.0  ...                  0.0     0.0     0.0      0.0     0.0     0.0      0.0      0.0      0.0                  0\n",
       "1  &TV Communications                                                                          |Games|         Games           40,00,000   operating          USA         CA    Los Angeles  Los Angeles             2.0  ...                  0.0     0.0     0.0      0.0     0.0     0.0      0.0      0.0      0.0                  0\n",
       "2   'Rock' Your Paper                                                           |Publishing|Education|    Publishing              40,000   operating          EST    Unknown        Tallinn      Tallinn             1.0  ...                  0.0     0.0     0.0      0.0     0.0     0.0      0.0      0.0      0.0                  0\n",
       "3   (In)Touch Network  |Electronics|Guides|Coffee|Restaurants|Music|iPhone|Apps|Mobile|iOS|E-Commerce|   Electronics           15,00,000   operating          GBR    Unknown         London       London             1.0  ...                  0.0     0.0     0.0      0.0     0.0     0.0      0.0      0.0      0.0                  0\n",
       "4  -R- Ranch and Mine                                                    |Tourism|Entertainment|Games|       Tourism              60,000   operating          USA         TX         Dallas   Fort Worth             2.0  ...                  0.0     0.0     0.0      0.0     0.0     0.0      0.0      0.0      0.0                  0\n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data Wrangler 生成的单元格\n",
    "\"\"\"\n",
    "def clean_data(df_clean_1):\n",
    "    target_cols = [\n",
    "    'seed',              # 种子轮融资\n",
    "    'venture',           # 风险投资\n",
    "    'equity_crowdfunding', # 股权众筹\n",
    "    'undisclosed',       # 未披露融资\n",
    "    'convertible_note',  # 可转换债券融资\n",
    "    'debt_financing',    # 债务融资\n",
    "    'angel',             # 天使轮融资\n",
    "    'grant',             # 政府/机构赠款\n",
    "    'private_equity',    # 私募股权投资\n",
    "    'post_ipo_equity',   # 上市后股权融资\n",
    "    'post_ipo_debt',     # 上市后债务融资\n",
    "    'secondary_market',  # 二级市场交易\n",
    "    'product_crowdfunding', # 产品众筹\n",
    "    'round_A',           # A轮融资\n",
    "    'round_B',           # B轮融资\n",
    "    'round_C',           # C轮融资\n",
    "    'round_D',           # D轮融资\n",
    "    'round_E',           # E轮融资\n",
    "    'round_F',           # F轮融资\n",
    "    'round_G',           # G轮融资\n",
    "    'round_H'            # H轮融资\n",
    "]\n",
    "    df_clean_1[target_cols] = df_clean_1[target_cols].apply(np.log1p)\n",
    "    return df_clean_1\n",
    "\n",
    "df_clean_2 = clean_data(df_clean_1.copy())\n",
    "df_clean_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=df_clean_2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name,category_list, market , funding_total_usd ,status,country_code,state_code,region,city,funding_rounds,founded_at,founded_month,founded_quarter,founded_year,first_funding_at,last_funding_at,seed,venture,equity_crowdfunding,undisclosed,convertible_note,debt_financing,angel,grant,private_equity,post_ipo_equity,post_ipo_debt,secondary_market,product_crowdfunding,round_A,round_B,round_C,round_D,round_E,round_F,round_G,round_H,fund_info_missing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "status\n",
       "operating    41829\n",
       "Unknown       6170\n",
       "acquired      3692\n",
       "closed        2603\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(*df_final.columns,sep=',')\n",
    "df_final['status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "label='status'\n",
    "time_limit=3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250429_090600\"\n",
      "Verbosity: 3 (Detailed Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 24.4.0: Fri Apr 11 18:34:14 PDT 2025; root:xnu-11417.101.15~117/RELEASE_ARM64_T8122\n",
      "CPU Count:          8\n",
      "GPU Count:          0\n",
      "Memory Avail:       1.82 GB / 8.00 GB (22.7%)\n",
      "Disk Space Avail:   13.91 GB / 228.27 GB (6.1%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='experimental' : New in v1.2: Pre-trained foundation model + parallel fits. The absolute best accuracy without consideration for inference speed. Does not support GPU.\n",
      "\tpresets='best'         : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'         : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'         : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'       : Fast training time, ideal for initial prototyping.\n",
      "============ fit kwarg info ============\n",
      "User Specified kwargs:\n",
      "{'verbosity': 3}\n",
      "Full kwargs:\n",
      "{'_feature_generator_kwargs': None,\n",
      " '_save_bag_folds': None,\n",
      " 'ag_args': None,\n",
      " 'ag_args_ensemble': None,\n",
      " 'ag_args_fit': None,\n",
      " 'auto_stack': False,\n",
      " 'calibrate': 'auto',\n",
      " 'delay_bag_sets': False,\n",
      " 'ds_args': {'clean_up_fits': True,\n",
      "             'detection_time_frac': 0.25,\n",
      "             'enable_callbacks': False,\n",
      "             'enable_ray_logging': True,\n",
      "             'holdout_data': None,\n",
      "             'holdout_frac': 0.1111111111111111,\n",
      "             'memory_safe_fits': True,\n",
      "             'n_folds': 2,\n",
      "             'n_repeats': 1,\n",
      "             'validation_procedure': 'holdout'},\n",
      " 'excluded_model_types': None,\n",
      " 'feature_generator': 'auto',\n",
      " 'feature_prune_kwargs': None,\n",
      " 'holdout_frac': None,\n",
      " 'hyperparameter_tune_kwargs': None,\n",
      " 'included_model_types': None,\n",
      " 'keep_only_best': False,\n",
      " 'learning_curves': False,\n",
      " 'name_suffix': None,\n",
      " 'num_bag_folds': None,\n",
      " 'num_bag_sets': None,\n",
      " 'num_stack_levels': None,\n",
      " 'pseudo_data': None,\n",
      " 'raise_on_no_models_fitted': True,\n",
      " 'refit_full': False,\n",
      " 'save_bag_folds': None,\n",
      " 'save_space': False,\n",
      " 'set_best_to_refit_full': False,\n",
      " 'test_data': None,\n",
      " 'unlabeled_data': None,\n",
      " 'use_bag_holdout': False,\n",
      " 'verbosity': 3}\n",
      "========================================\n",
      "Saving /Users/zhaohongbo/PycharmProjects/Pytorch_learning/VC/AutogluonModels/ag-20250429_090600/learner.pkl\n",
      "Saving /Users/zhaohongbo/PycharmProjects/Pytorch_learning/VC/AutogluonModels/ag-20250429_090600/predictor.pkl\n",
      "Beginning AutoGluon training ... Time limit = 3600s\n",
      "AutoGluon will save models to \"/Users/zhaohongbo/PycharmProjects/Pytorch_learning/VC/AutogluonModels/ag-20250429_090600\"\n",
      "Train Data Rows:    54294\n",
      "Train Data Columns: 37\n",
      "Label Column:       status\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 4\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1868.32 MB\n",
      "\tTrain Data (Original)  Memory Usage: 53.86 MB (2.9% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\t\t\tOriginal Features (exact raw dtype, raw dtype):\n",
      "\t\t\t\t('float64', 'float') : 23 | ['funding_rounds', 'founded_year', 'seed', 'venture', 'equity_crowdfunding', ...]\n",
      "\t\t\t\t('int64', 'int')     :  1 | ['fund_info_missing']\n",
      "\t\t\t\t('object', 'object') : 13 | ['name', 'category_list', ' market ', ' funding_total_usd ', 'country_code', ...]\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])                      : 23 | ['funding_rounds', 'founded_year', 'seed', 'venture', 'equity_crowdfunding', ...]\n",
      "\t\t\t\t('int', [])                        :  1 | ['fund_info_missing']\n",
      "\t\t\t\t('object', [])                     :  8 | ['name', 'category_list', ' market ', ' funding_total_usd ', 'country_code', ...]\n",
      "\t\t\t\t('object', ['datetime_as_object']) :  5 | ['founded_at', 'founded_month', 'founded_quarter', 'first_funding_at', 'last_funding_at']\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])                      : 23 | ['funding_rounds', 'founded_year', 'seed', 'venture', 'equity_crowdfunding', ...]\n",
      "\t\t\t\t('int', ['bool'])                  :  1 | ['fund_info_missing']\n",
      "\t\t\t\t('object', [])                     :  8 | ['name', 'category_list', ' market ', ' funding_total_usd ', 'country_code', ...]\n",
      "\t\t\t\t('object', ['datetime_as_object']) :  5 | ['founded_at', 'founded_month', 'founded_quarter', 'first_funding_at', 'last_funding_at']\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t37 features in original data used to generate 37 features in processed data.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])                      : 23 | ['funding_rounds', 'founded_year', 'seed', 'venture', 'equity_crowdfunding', ...]\n",
      "\t\t\t\t('int', ['bool'])                  :  1 | ['fund_info_missing']\n",
      "\t\t\t\t('object', [])                     :  8 | ['name', 'category_list', ' market ', ' funding_total_usd ', 'country_code', ...]\n",
      "\t\t\t\t('object', ['datetime_as_object']) :  5 | ['founded_at', 'founded_month', 'founded_quarter', 'first_funding_at', 'last_funding_at']\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])                      : 23 | ['funding_rounds', 'founded_year', 'seed', 'venture', 'equity_crowdfunding', ...]\n",
      "\t\t\t\t('int', ['bool'])                  :  1 | ['fund_info_missing']\n",
      "\t\t\t\t('object', [])                     :  8 | ['name', 'category_list', ' market ', ' funding_total_usd ', 'country_code', ...]\n",
      "\t\t\t\t('object', ['datetime_as_object']) :  5 | ['founded_at', 'founded_month', 'founded_quarter', 'first_funding_at', 'last_funding_at']\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t37 features in original data used to generate 37 features in processed data.\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 23 | ['funding_rounds', 'founded_year', 'seed', 'venture', 'equity_crowdfunding', ...]\n",
      "\t\t\t\t('int', ['bool']) :  1 | ['fund_info_missing']\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 23 | ['funding_rounds', 'founded_year', 'seed', 'venture', 'equity_crowdfunding', ...]\n",
      "\t\t\t\t('int', ['bool']) :  1 | ['fund_info_missing']\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t24 features in original data used to generate 24 features in processed data.\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t\t('category', []) : 8 | ['name', 'category_list', ' market ', ' funding_total_usd ', 'country_code', ...]\n",
      "\t\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t\t('category', []) : 8 | ['name', 'category_list', ' market ', ' funding_total_usd ', 'country_code', ...]\n",
      "\t\t\t\t0.0s = Fit runtime\n",
      "\t\t\t\t8 features in original data used to generate 8 features in processed data.\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('object', []) : 8 | ['name', 'category_list', ' market ', ' funding_total_usd ', 'country_code', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', []) : 8 | ['name', 'category_list', ' market ', ' funding_total_usd ', 'country_code', ...]\n",
      "\t\t\t0.1s = Fit runtime\n",
      "\t\t\t8 features in original data used to generate 8 features in processed data.\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('object', ['datetime_as_object']) : 5 | ['founded_at', 'founded_month', 'founded_quarter', 'first_funding_at', 'last_funding_at']\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('int', ['datetime_as_int']) : 25 | ['founded_at', 'founded_at.year', 'founded_at.month', 'founded_at.day', 'founded_at.dayofweek', ...]\n",
      "\t\t\t0.1s = Fit runtime\n",
      "\t\t\t5 features in original data used to generate 25 features in processed data.\n",
      "\t\tSkipping TextSpecialFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping TextNgramFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IdentityFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IsNanFeatureGenerator: No input feature with required dtypes.\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])             :  8 | ['name', 'category_list', ' market ', ' funding_total_usd ', 'country_code', ...]\n",
      "\t\t\t\t('float', [])                : 23 | ['funding_rounds', 'founded_year', 'seed', 'venture', 'equity_crowdfunding', ...]\n",
      "\t\t\t\t('int', ['bool'])            :  1 | ['fund_info_missing']\n",
      "\t\t\t\t('int', ['datetime_as_int']) : 25 | ['founded_at', 'founded_at.year', 'founded_at.month', 'founded_at.day', 'founded_at.dayofweek', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])             :  8 | ['name', 'category_list', ' market ', ' funding_total_usd ', 'country_code', ...]\n",
      "\t\t\t\t('float', [])                : 23 | ['funding_rounds', 'founded_year', 'seed', 'venture', 'equity_crowdfunding', ...]\n",
      "\t\t\t\t('int', ['bool'])            :  1 | ['fund_info_missing']\n",
      "\t\t\t\t('int', ['datetime_as_int']) : 25 | ['founded_at', 'founded_at.year', 'founded_at.month', 'founded_at.day', 'founded_at.dayofweek', ...]\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t57 features in original data used to generate 57 features in processed data.\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\t\t2 duplicate columns removed: ['round_H', 'founded_quarter.year']\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])             :  8 | ['name', 'category_list', ' market ', ' funding_total_usd ', 'country_code', ...]\n",
      "\t\t\t\t('float', [])                : 22 | ['funding_rounds', 'founded_year', 'seed', 'venture', 'equity_crowdfunding', ...]\n",
      "\t\t\t\t('int', ['bool'])            :  1 | ['fund_info_missing']\n",
      "\t\t\t\t('int', ['datetime_as_int']) : 24 | ['founded_at', 'founded_at.year', 'founded_at.month', 'founded_at.day', 'founded_at.dayofweek', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])             :  8 | ['name', 'category_list', ' market ', ' funding_total_usd ', 'country_code', ...]\n",
      "\t\t\t\t('float', [])                : 22 | ['funding_rounds', 'founded_year', 'seed', 'venture', 'equity_crowdfunding', ...]\n",
      "\t\t\t\t('int', ['bool'])            :  1 | ['fund_info_missing']\n",
      "\t\t\t\t('int', ['datetime_as_int']) : 24 | ['founded_at', 'founded_at.year', 'founded_at.month', 'founded_at.day', 'founded_at.dayofweek', ...]\n",
      "\t\t\t0.2s = Fit runtime\n",
      "\t\t\t55 features in original data used to generate 55 features in processed data.\n",
      "\tUnused Original Features (Count: 1): ['round_H']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 1 | ['round_H']\n",
      "\tTypes of features in original data (exact raw dtype, raw dtype):\n",
      "\t\t('float64', 'float') : 22 | ['funding_rounds', 'founded_year', 'seed', 'venture', 'equity_crowdfunding', ...]\n",
      "\t\t('int64', 'int')     :  1 | ['fund_info_missing']\n",
      "\t\t('object', 'object') : 13 | ['name', 'category_list', ' market ', ' funding_total_usd ', 'country_code', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 22 | ['funding_rounds', 'founded_year', 'seed', 'venture', 'equity_crowdfunding', ...]\n",
      "\t\t('int', [])                        :  1 | ['fund_info_missing']\n",
      "\t\t('object', [])                     :  8 | ['name', 'category_list', ' market ', ' funding_total_usd ', 'country_code', ...]\n",
      "\t\t('object', ['datetime_as_object']) :  5 | ['founded_at', 'founded_month', 'founded_quarter', 'first_funding_at', 'last_funding_at']\n",
      "\tTypes of features in processed data (exact raw dtype, raw dtype):\n",
      "\t\t('category', 'category') :  8 | ['name', 'category_list', ' market ', ' funding_total_usd ', 'country_code', ...]\n",
      "\t\t('float64', 'float')     : 22 | ['funding_rounds', 'founded_year', 'seed', 'venture', 'equity_crowdfunding', ...]\n",
      "\t\t('int64', 'int')         : 24 | ['founded_at', 'founded_at.year', 'founded_at.month', 'founded_at.day', 'founded_at.dayofweek', ...]\n",
      "\t\t('int8', 'int')          :  1 | ['fund_info_missing']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])             :  8 | ['name', 'category_list', ' market ', ' funding_total_usd ', 'country_code', ...]\n",
      "\t\t('float', [])                : 22 | ['funding_rounds', 'founded_year', 'seed', 'venture', 'equity_crowdfunding', ...]\n",
      "\t\t('int', ['bool'])            :  1 | ['fund_info_missing']\n",
      "\t\t('int', ['datetime_as_int']) : 24 | ['founded_at', 'founded_at.year', 'founded_at.month', 'founded_at.day', 'founded_at.dayofweek', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t36 features in original data used to generate 55 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 19.78 MB (1.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.72s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Saving /Users/zhaohongbo/PycharmProjects/Pytorch_learning/VC/AutogluonModels/ag-20250429_090600/learner.pkl\n",
      "Automatically generating train/validation split with holdout_frac=0.04604560356577154, Train Rows: 51794, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Saving /Users/zhaohongbo/PycharmProjects/Pytorch_learning/VC/AutogluonModels/ag-20250429_090600/utils/data/X.pkl\n",
      "Saving /Users/zhaohongbo/PycharmProjects/Pytorch_learning/VC/AutogluonModels/ag-20250429_090600/utils/data/y.pkl\n",
      "Saving /Users/zhaohongbo/PycharmProjects/Pytorch_learning/VC/AutogluonModels/ag-20250429_090600/utils/data/X_val.pkl\n",
      "Saving /Users/zhaohongbo/PycharmProjects/Pytorch_learning/VC/AutogluonModels/ag-20250429_090600/utils/data/y_val.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tKNeighborsUnif: \t{'weights': 'uniform', 'ag_args': {'valid_stacker': False, 'problem_types': ['binary', 'multiclass', 'regression'], 'name_suffix': 'Unif', 'model_type': <class 'autogluon.tabular.models.knn.knn_model.KNNModel'>, 'priority': 100}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tKNeighborsDist: \t{'weights': 'distance', 'ag_args': {'valid_stacker': False, 'problem_types': ['binary', 'multiclass', 'regression'], 'name_suffix': 'Dist', 'model_type': <class 'autogluon.tabular.models.knn.knn_model.KNNModel'>, 'priority': 100}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tNeuralNetFastAI: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>, 'priority': 95}}\n",
      "\tLightGBMXT: \t{'extra_trees': True, 'ag_args': {'name_suffix': 'XT', 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "\tLightGBM: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "\tRandomForestGini: \t{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass'], 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tRandomForestEntr: \t{'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass'], 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tCatBoost: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>, 'priority': 70}}\n",
      "\tExtraTreesGini: \t{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass'], 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tExtraTreesEntr: \t{'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass'], 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tXGBoost: \t{'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>, 'priority': 40}}\n",
      "\tNeuralNetTorch: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>, 'priority': 25}}\n",
      "\tLightGBMLarge: \t{'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}}\n",
      "Fitting 13 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 3599.28s of the 3599.27s of remaining time.\n",
      "\tFitting KNeighborsUnif with 'num_gpus': 0, 'num_cpus': 8\n",
      "\t0.01s \t= Train Time (Using 10000/51794 rows) (3599.23s remaining time)\n",
      "\t0.01s \t= Train Time (Using 20000/51794 rows) (3599.22s remaining time)\n",
      "\t0.0s \t= Train Time (Using 51794/51794 rows) (3599.22s remaining time)\n",
      "Saving /Users/zhaohongbo/PycharmProjects/Pytorch_learning/VC/AutogluonModels/ag-20250429_090600/models/KNeighborsUnif/model.pkl\n",
      "Saving /Users/zhaohongbo/PycharmProjects/Pytorch_learning/VC/AutogluonModels/ag-20250429_090600/utils/attr/KNeighborsUnif/y_pred_proba_val.pkl\n",
      "\t0.5475\t = Validation score   (f1_macro)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "\t11615.1\t = Inference  throughput (rows/s | 2500 batch size)\n",
      "Saving /Users/zhaohongbo/PycharmProjects/Pytorch_learning/VC/AutogluonModels/ag-20250429_090600/models/trainer.pkl\n",
      "Fitting model: KNeighborsDist ... Training model for up to 3598.88s of the 3598.88s of remaining time.\n",
      "\tFitting KNeighborsDist with 'num_gpus': 0, 'num_cpus': 8\n",
      "\t0.02s \t= Train Time (Using 10000/51794 rows) (3598.83s remaining time)\n",
      "\t0.01s \t= Train Time (Using 20000/51794 rows) (3598.82s remaining time)\n",
      "\t0.01s \t= Train Time (Using 51794/51794 rows) (3598.82s remaining time)\n",
      "Saving /Users/zhaohongbo/PycharmProjects/Pytorch_learning/VC/AutogluonModels/ag-20250429_090600/models/KNeighborsDist/model.pkl\n",
      "Saving /Users/zhaohongbo/PycharmProjects/Pytorch_learning/VC/AutogluonModels/ag-20250429_090600/utils/attr/KNeighborsDist/y_pred_proba_val.pkl\n",
      "\t0.5258\t = Validation score   (f1_macro)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "\t13525.1\t = Inference  throughput (rows/s | 2500 batch size)\n",
      "Saving /Users/zhaohongbo/PycharmProjects/Pytorch_learning/VC/AutogluonModels/ag-20250429_090600/models/trainer.pkl\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 3598.62s of the 3598.61s of remaining time.\n",
      "\tFitting NeuralNetFastAI with 'num_gpus': 0, 'num_cpus': 8\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.2`. \n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/common/utils/try_import.py\", line 134, in try_import_fastai\n",
      "    import fastai\n",
      "ModuleNotFoundError: No module named 'fastai'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 219, in _fit\n",
      "    try_import_fastai()\n",
      "  File \"/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/common/utils/try_import.py\", line 147, in try_import_fastai\n",
      "    raise ImportError(\n",
      "ImportError: Import fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.2`. \n",
      "Saving /Users/zhaohongbo/PycharmProjects/Pytorch_learning/VC/AutogluonModels/ag-20250429_090600/models/trainer.pkl\n",
      "Fitting model: LightGBMXT ... Training model for up to 3597.95s of the 3597.95s of remaining time.\n",
      "\tFitting LightGBMXT with 'num_gpus': 0, 'num_cpus': 8\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.2`.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/common/utils/try_import.py\", line 93, in try_import_lightgbm\n",
      "    import lightgbm\n",
      "ModuleNotFoundError: No module named 'lightgbm'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 114, in _fit\n",
      "    try_import_lightgbm()  # raise helpful error message if LightGBM isn't installed\n",
      "  File \"/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/common/utils/try_import.py\", line 95, in try_import_lightgbm\n",
      "    raise ImportError(\n",
      "ImportError: `import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.2`.\n",
      "Saving /Users/zhaohongbo/PycharmProjects/Pytorch_learning/VC/AutogluonModels/ag-20250429_090600/models/trainer.pkl\n",
      "Fitting model: LightGBM ... Training model for up to 3597.80s of the 3597.79s of remaining time.\n",
      "\tFitting LightGBM with 'num_gpus': 0, 'num_cpus': 8\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.2`.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/common/utils/try_import.py\", line 93, in try_import_lightgbm\n",
      "    import lightgbm\n",
      "ModuleNotFoundError: No module named 'lightgbm'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 114, in _fit\n",
      "    try_import_lightgbm()  # raise helpful error message if LightGBM isn't installed\n",
      "  File \"/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/common/utils/try_import.py\", line 95, in try_import_lightgbm\n",
      "    raise ImportError(\n",
      "ImportError: `import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.2`.\n",
      "Saving /Users/zhaohongbo/PycharmProjects/Pytorch_learning/VC/AutogluonModels/ag-20250429_090600/models/trainer.pkl\n",
      "Fitting model: RandomForestGini ... Training model for up to 3597.65s of the 3597.64s of remaining time.\n",
      "\tFitting RandomForestGini with 'num_gpus': 0, 'num_cpus': 8\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 208 due to low memory. Expected memory usage reduced from 21.57% -> 15.0% of available memory...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mTabularPredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproblem_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmulticlass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mf1_macro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_final\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/core/utils/decorators.py:31\u001b[0m, in \u001b[0;36munpack.<locals>._unpack_inner.<locals>._call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     30\u001b[0m     gargs, gkwargs \u001b[38;5;241m=\u001b[39m g(\u001b[38;5;241m*\u001b[39mother_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/tabular/predictor/predictor.py:1299\u001b[0m, in \u001b[0;36mTabularPredictor.fit\u001b[0;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, fit_full_last_level_weighted_ensemble, full_weighted_ensemble_additionally, dynamic_stacking, calibrate_decision_threshold, num_cpus, num_gpus, fit_strategy, memory_limit, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;66;03m# keep track of the fit strategy used for future calls\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_strategy \u001b[38;5;241m=\u001b[39m fit_strategy\n\u001b[0;32m-> 1299\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/tabular/predictor/predictor.py:1305\u001b[0m, in \u001b[0;36mTabularPredictor._fit\u001b[0;34m(self, ag_fit_kwargs, ag_post_fit_kwargs)\u001b[0m\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, ag_fit_kwargs: \u001b[38;5;28mdict\u001b[39m, ag_post_fit_kwargs: \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Save predictor to disk to enable prediction and training after interrupt\u001b[39;00m\n\u001b[0;32m-> 1305\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_learner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1306\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_post_fit_vars()\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_fit(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag_post_fit_kwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/tabular/learner/abstract_learner.py:159\u001b[0m, in \u001b[0;36mAbstractTabularLearner.fit\u001b[0;34m(self, X, X_val, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearner is already fit.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_fit_input(X\u001b[38;5;241m=\u001b[39mX, X_val\u001b[38;5;241m=\u001b[39mX_val, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/tabular/learner/default_learner.py:131\u001b[0m, in \u001b[0;36mDefaultLearner._fit\u001b[0;34m(self, X, X_val, X_test, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, infer_limit, infer_limit_batch_size, verbosity, **trainer_fit_kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_metric \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39meval_metric\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m--> 131\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mholdout_frac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mholdout_frac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit_trainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrainer_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_trainer(trainer\u001b[38;5;241m=\u001b[39mtrainer)\n\u001b[1;32m    147\u001b[0m time_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/tabular/trainer/auto_trainer.py:135\u001b[0m, in \u001b[0;36mAutoTrainer.fit\u001b[0;34m(self, X, y, hyperparameters, X_val, y_val, X_test, y_test, X_unlabeled, holdout_frac, num_stack_levels, core_kwargs, aux_kwargs, time_limit, infer_limit, infer_limit_batch_size, use_bag_holdout, groups, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m log_str \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m20\u001b[39m, log_str)\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_multi_and_ensemble\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_stack_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_stack_levels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcore_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcore_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43maux_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maux_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py:3238\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_and_ensemble\u001b[0;34m(self, X, y, X_val, y_val, X_test, y_test, hyperparameters, X_unlabeled, num_stack_levels, time_limit, groups, **kwargs)\u001b[0m\n\u001b[1;32m   3236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_rows_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_test)\n\u001b[1;32m   3237\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_cols_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(X\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[0;32m-> 3238\u001b[0m model_names_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_multi_levels\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3240\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3243\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3245\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_stack_levels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3251\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model_names()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3253\u001b[0m     \u001b[38;5;66;03m# TODO v1.0: Add toggle to raise exception if no models trained\u001b[39;00m\n\u001b[1;32m   3254\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m30\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: AutoGluon did not successfully train any models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py:493\u001b[0m, in \u001b[0;36mAbstractTrainer.train_multi_levels\u001b[0;34m(self, X, y, hyperparameters, X_val, y_val, X_test, y_test, X_unlabeled, base_model_names, core_kwargs, aux_kwargs, level_start, level_end, time_limit, name_suffix, relative_stack, level_time_modifier, infer_limit, infer_limit_batch_size, callbacks)\u001b[0m\n\u001b[1;32m    491\u001b[0m         core_kwargs_level[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m core_kwargs_level\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m, time_limit_core)\n\u001b[1;32m    492\u001b[0m         aux_kwargs_level[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m aux_kwargs_level\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m, time_limit_aux)\n\u001b[0;32m--> 493\u001b[0m     base_model_names, aux_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack_new_level\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_model_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_model_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcore_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcore_kwargs_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43maux_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maux_kwargs_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_suffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname_suffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfull_weighted_ensemble\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_weighted_ensemble\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_full_weighted_ensemble\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_full_weighted_ensemble\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    512\u001b[0m     model_names_fit \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m base_model_names \u001b[38;5;241m+\u001b[39m aux_models\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_best \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m infer_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model_names_fit) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py:688\u001b[0m, in \u001b[0;36mAbstractTrainer.stack_new_level\u001b[0;34m(self, X, y, models, X_val, y_val, X_test, y_test, X_unlabeled, level, base_model_names, core_kwargs, aux_kwargs, name_suffix, infer_limit, infer_limit_batch_size, full_weighted_ensemble, additional_full_weighted_ensemble)\u001b[0m\n\u001b[1;32m    686\u001b[0m     core_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m core_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m name_suffix\n\u001b[1;32m    687\u001b[0m     aux_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m aux_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m name_suffix\n\u001b[0;32m--> 688\u001b[0m core_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack_new_level_core\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_model_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcore_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m aux_models \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_weighted_ensemble:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py:835\u001b[0m, in \u001b[0;36mAbstractTrainer.stack_new_level_core\u001b[0;34m(self, X, y, models, X_val, y_val, X_test, y_test, X_unlabeled, level, base_model_names, fit_strategy, stack_name, ag_args, ag_args_fit, ag_args_ensemble, included_model_types, excluded_model_types, ensemble_type, name_suffix, get_models_func, refit_full, infer_limit, infer_limit_batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    829\u001b[0m fit_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    830\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes,\n\u001b[1;32m    831\u001b[0m     feature_metadata\u001b[38;5;241m=\u001b[39mfeature_metadata,\n\u001b[1;32m    832\u001b[0m )\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# FIXME: TODO: v0.1 X_unlabeled isn't cached so it won't be available during refit_full or fit_extra.\u001b[39;00m\n\u001b[0;32m--> 835\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_multi\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstack_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstack_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py:3170\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi\u001b[0;34m(self, X, y, models, hyperparameter_tune_kwargs, feature_prune_kwargs, k_fold, n_repeats, n_repeat_start, time_limit, delay_bag_sets, **kwargs)\u001b[0m\n\u001b[1;32m   3168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_repeat_start \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3169\u001b[0m     time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 3170\u001b[0m     model_names_trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_multi_initial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3172\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk_fold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk_fold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_repeats_initial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_prune_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_prune_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3179\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3180\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3181\u001b[0m     n_repeat_start \u001b[38;5;241m=\u001b[39m n_repeats_initial\n\u001b[1;32m   3182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py:2755\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_initial\u001b[0;34m(self, X, y, models, k_fold, n_repeats, hyperparameter_tune_kwargs, time_limit, feature_prune_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   2753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m bagged:\n\u001b[1;32m   2754\u001b[0m     time_ratio \u001b[38;5;241m=\u001b[39m hpo_time_ratio \u001b[38;5;28;01mif\u001b[39;00m hpo_enabled \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2755\u001b[0m     models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_multi_fold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2758\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2761\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2762\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2763\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2764\u001b[0m     time_ratio \u001b[38;5;241m=\u001b[39m hpo_time_ratio \u001b[38;5;28;01mif\u001b[39;00m hpo_enabled \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py:2922\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_fold\u001b[0;34m(self, X, y, models, time_limit, time_split, time_ratio, hyperparameter_tune_kwargs, fit_strategy, **kwargs)\u001b[0m\n\u001b[1;32m   2919\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_early_stop:\n\u001b[1;32m   2920\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m models_valid\n\u001b[0;32m-> 2922\u001b[0m         models_valid \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m_detached_train_multi_fold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2923\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_self\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2924\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2925\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2926\u001b[0m \u001b[43m            \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2927\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2928\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2929\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2930\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_limit_model_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit_model_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2931\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2932\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_ray_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2933\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2934\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2935\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fit_strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2936\u001b[0m     models_valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_multi_fold_parallel(\n\u001b[1;32m   2937\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   2938\u001b[0m         y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2945\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2946\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py:4621\u001b[0m, in \u001b[0;36m_detached_train_multi_fold\u001b[0;34m(_self, model, X, y, time_split, time_start, time_limit, time_limit_model_split, hyperparameter_tune_kwargs, is_ray_worker, kwargs)\u001b[0m\n\u001b[1;32m   4618\u001b[0m         time_start_model\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   4619\u001b[0m         time_left\u001b[38;5;241m=\u001b[39mtime_limit\u001b[38;5;241m-\u001b[39m(time_start_model\u001b[38;5;241m-\u001b[39mtime_start)\n\u001b[0;32m-> 4621\u001b[0m model_name_trained_lst \u001b[38;5;241m=\u001b[39m \u001b[43m_self\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_single_full\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4623\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_left\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameter_tune_kwargs_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_ray_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_ray_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   4629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _self\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m   4632\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py:2538\u001b[0m, in \u001b[0;36mAbstractTrainer._train_single_full\u001b[0;34m(self, X, y, model, X_unlabeled, X_val, y_val, X_test, y_test, X_pseudo, y_pseudo, feature_prune, hyperparameter_tune_kwargs, stack_name, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, level, time_limit, fit_kwargs, compute_score, total_resources, errors, errors_ignore, errors_raise, is_ray_worker, **kwargs)\u001b[0m\n\u001b[1;32m   2534\u001b[0m         bagged_model_fit_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_bagged_model_fit_kwargs(\n\u001b[1;32m   2535\u001b[0m             k_fold\u001b[38;5;241m=\u001b[39mk_fold, k_fold_start\u001b[38;5;241m=\u001b[39mk_fold_start, k_fold_end\u001b[38;5;241m=\u001b[39mk_fold_end, n_repeats\u001b[38;5;241m=\u001b[39mn_repeats, n_repeat_start\u001b[38;5;241m=\u001b[39mn_repeat_start\n\u001b[1;32m   2536\u001b[0m         )\n\u001b[1;32m   2537\u001b[0m         model_fit_kwargs\u001b[38;5;241m.\u001b[39mupdate(bagged_model_fit_kwargs)\n\u001b[0;32m-> 2538\u001b[0m     model_names_trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_and_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2540\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2545\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstack_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstack_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompute_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_resources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_resources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2551\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2552\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors_ignore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors_ignore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2553\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors_raise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors_raise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_ray_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_ray_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2555\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;129;01mand\u001b[39;00m check_callbacks:\n\u001b[1;32m   2558\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callbacks_after_fit(model_names\u001b[38;5;241m=\u001b[39mmodel_names_trained, stack_name\u001b[38;5;241m=\u001b[39mstack_name, level\u001b[38;5;241m=\u001b[39mlevel)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py:2106\u001b[0m, in \u001b[0;36mAbstractTrainer._train_and_save\u001b[0;34m(self, X, y, model, X_val, y_val, X_test, y_test, X_pseudo, y_pseudo, time_limit, stack_name, level, compute_score, total_resources, errors, errors_ignore, errors_raise, is_ray_worker, **model_fit_kwargs)\u001b[0m\n\u001b[1;32m   2104\u001b[0m exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2106\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m     fit_end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   2109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_evaluation:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py:1993\u001b[0m, in \u001b[0;36mAbstractTrainer._train_single\u001b[0;34m(self, X, y, model, X_val, y_val, X_test, y_test, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[1;32m   1977\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_train_single\u001b[39m(\n\u001b[1;32m   1978\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1979\u001b[0m     X: pd\u001b[38;5;241m.\u001b[39mDataFrame,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1987\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_fit_kwargs,\n\u001b[1;32m   1988\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AbstractModel:\n\u001b[1;32m   1989\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1990\u001b[0m \u001b[38;5;124;03m    Trains model but does not add the trained model to this Trainer.\u001b[39;00m\n\u001b[1;32m   1991\u001b[0m \u001b[38;5;124;03m    Returns trained model object.\u001b[39;00m\n\u001b[1;32m   1992\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1993\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_resources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_resources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1994\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py:925\u001b[0m, in \u001b[0;36mAbstractModel.fit\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mWarning: Model has no time left to train, skipping model... (Time Left = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    924\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TimeLimitExceeded\n\u001b[0;32m--> 925\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    927\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/autogluon/tabular/models/rf/rf_model.py:214\u001b[0m, in \u001b[0;36mRFModel._fit\u001b[0;34m(self, X, y, num_cpus, time_limit, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m n_estimators\n\u001b[1;32m    213\u001b[0m         model \u001b[38;5;241m=\u001b[39m model_cls(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 214\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(n_estimator_increments) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    216\u001b[0m     time_elapsed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m time_train_start, \u001b[38;5;241m0.001\u001b[39m)  \u001b[38;5;66;03m# avoid it being too small and being truncated to 0\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    481\u001b[0m ]\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Pytorch_learning/lib/python3.9/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(label=label, problem_type='multiclass',eval_metric='f1_macro',).fit(\n",
    "    train_data=df_final,\n",
    "    time_limit=time_limit,\n",
    "    verbosity=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'autogluon.tabular.predictor' has no attribute 'leaderboard'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleaderboard\u001b[49m(df_final, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'autogluon.tabular.predictor' has no attribute 'leaderboard'"
     ]
    }
   ],
   "source": [
    "predictor.leaderboard(df_final, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: g:\\PythonProjects\\Pytorch_learning\\Pytorch_learning\\VC\\AutogluonModels\\ag-20250429_071012\\models\\LightGBM\\model.pkl\n",
      "Loading: g:\\PythonProjects\\Pytorch_learning\\Pytorch_learning\\VC\\AutogluonModels\\ag-20250429_071012\\models\\NeuralNetTorch\\model.pkl\n",
      "Loading: g:\\PythonProjects\\Pytorch_learning\\Pytorch_learning\\VC\\AutogluonModels\\ag-20250429_071012\\models\\WeightedEnsemble_L2\\model.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1_macro': 0.764495599831698,\n",
       " 'accuracy': 0.9094375069068406,\n",
       " 'balanced_accuracy': 0.7238629106057288,\n",
       " 'mcc': 0.7531324962917333}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.evaluate(df_final, auxiliary_metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'autogluon.tabular.predictor' has no attribute 'feature_importance'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m feature_data\u001b[38;5;241m=\u001b[39m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_importance\u001b[49m(df_final)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'autogluon.tabular.predictor' has no attribute 'feature_importance'"
     ]
    }
   ],
   "source": [
    "feature_data=predictor.feature_importance(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
